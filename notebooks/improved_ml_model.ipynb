{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Improved ML Model for Heat Flux Prediction\n",
        "\n",
        "Based on initial results (30.8% within ±5%), implementing improvements:\n",
        "1. **Larger, deeper network** - More capacity for complex relationships\n",
        "2. **Better feature engineering** - Add derived physics features  \n",
        "3. **Ensemble methods** - Combine multiple models\n",
        "4. **Advanced regularization** - Better generalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU'))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data (reuse cleaned data)\n",
        "print(\"=== LOADING CLEAN DATA ===\")\n",
        "\n",
        "# Load the original data\n",
        "df = pd.read_csv('../Data/raw/apollo_cfd_database.csv')\n",
        "\n",
        "# Apply same cleaning as before\n",
        "df_clean = df.copy()\n",
        "df_clean = df_clean[df_clean['theta (m)'] >= 0]\n",
        "df_clean = df_clean[df_clean['Re-theta'] >= 1e-5]\n",
        "\n",
        "# Apply log transformations\n",
        "df_clean['log_density'] = np.log10(df_clean['density (kg/m^3)'])\n",
        "df_clean['log_velocity'] = np.log10(df_clean['velocity (m/s)'])\n",
        "\n",
        "print(f\"Clean dataset: {len(df_clean):,} points\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Feature Engineering - Key Improvement #1\n",
        "print(\"=== ENHANCED FEATURE ENGINEERING ===\")\n",
        "\n",
        "# Add physics-based derived features\n",
        "df_clean['log_dynamic_pressure'] = np.log10(df_clean['dynamic_pressure (Pa)'])\n",
        "df_clean['log_pressure'] = np.log10(df_clean['pw (Pa)'])\n",
        "\n",
        "# Velocity-based features (heat flux ∝ velocity^n)\n",
        "df_clean['velocity_cubed'] = df_clean['velocity (m/s)']**3  # Sutton-Graves relation\n",
        "df_clean['sqrt_velocity'] = np.sqrt(df_clean['velocity (m/s)'])\n",
        "\n",
        "# Density-velocity interactions\n",
        "df_clean['rho_v_squared'] = df_clean['density (kg/m^3)'] * df_clean['velocity (m/s)']**2\n",
        "\n",
        "# Spatial features (distance from stagnation point)\n",
        "df_clean['distance_from_center'] = np.sqrt(df_clean['X']**2 + df_clean['Y']**2 + df_clean['Z']**2)\n",
        "df_clean['x_normalized'] = df_clean['X'] / df_clean['distance_from_center']\n",
        "df_clean['y_normalized'] = df_clean['Y'] / df_clean['distance_from_center']\n",
        "df_clean['z_normalized'] = df_clean['Z'] / df_clean['distance_from_center']\n",
        "\n",
        "# Enhanced input features (14 total vs original 6)\n",
        "enhanced_features = [\n",
        "    'log_density', 'log_velocity', 'aoa (degrees)',\n",
        "    'log_dynamic_pressure', 'log_pressure',\n",
        "    'X', 'Y', 'Z', 'distance_from_center',\n",
        "    'x_normalized', 'y_normalized', 'z_normalized',\n",
        "    'velocity_cubed', 'rho_v_squared'\n",
        "]\n",
        "\n",
        "target_variable = 'qw (W/m^2)'\n",
        "\n",
        "print(f\"Enhanced features ({len(enhanced_features)}): {enhanced_features}\")\n",
        "print(f\"Target: {target_variable}\")\n",
        "print(f\"Feature expansion: 6 → {len(enhanced_features)} features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Improved data preparation\n",
        "print(\"=== IMPROVED DATA PREPARATION ===\")\n",
        "\n",
        "# Check which features actually exist in the dataframe\n",
        "available_features = [f for f in enhanced_features if f in df_clean.columns]\n",
        "missing_features = [f for f in enhanced_features if f not in df_clean.columns]\n",
        "\n",
        "if missing_features:\n",
        "    print(f\"Missing features: {missing_features}\")\n",
        "    print(\"Using available features only...\")\n",
        "    enhanced_features = available_features\n",
        "\n",
        "print(f\"Using {len(enhanced_features)} features: {enhanced_features}\")\n",
        "\n",
        "# Remove any infinite or NaN values from available features\n",
        "df_clean = df_clean.replace([np.inf, -np.inf], np.nan).dropna(subset=enhanced_features + [target_variable])\n",
        "print(f\"After removing invalid values: {len(df_clean):,} points\")\n",
        "\n",
        "# Prepare feature matrix and target\n",
        "X = df_clean[enhanced_features].values\n",
        "y = df_clean[target_variable].values\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Split data: 80% train, 10% val, 10% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"Training: {X_train.shape[0]:,} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Validation: {X_val.shape[0]:,} ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Test: {X_test.shape[0]:,} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced preprocessing - Key Improvement #2\n",
        "print(\"=== ADVANCED PREPROCESSING ===\")\n",
        "\n",
        "# Use RobustScaler (less sensitive to outliers) instead of StandardScaler\n",
        "scaler_X = RobustScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "# Log transform and robust scale targets\n",
        "y_train_log = np.log10(y_train)\n",
        "y_val_log = np.log10(y_val)\n",
        "y_test_log = np.log10(y_test)\n",
        "\n",
        "scaler_y = RobustScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_log.reshape(-1, 1)).flatten()\n",
        "y_val_scaled = scaler_y.transform(y_val_log.reshape(-1, 1)).flatten()\n",
        "y_test_scaled = scaler_y.transform(y_test_log.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"RobustScaler applied to {X_train_scaled.shape[1]} features\")\n",
        "print(f\"Target scaling - Log range: {y_train_log.min():.3f} to {y_train_log.max():.3f}\")\n",
        "print(f\"Target scaling - Scaled range: {y_train_scaled.min():.3f} to {y_train_scaled.max():.3f}\")\n",
        "print(\"✅ Advanced preprocessing complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the missing critical features\n",
        "print(\"=== ADDING MISSING CRITICAL FEATURES ===\")\n",
        "\n",
        "# Add the NASA baseline features that were missing\n",
        "df_clean['log_density'] = np.log10(df_clean['density (kg/m^3)'])\n",
        "df_clean['log_velocity'] = np.log10(df_clean['velocity (m/s)'])\n",
        "\n",
        "# Now use the complete enhanced feature set\n",
        "complete_enhanced_features = [\n",
        "    'log_density', 'log_velocity', 'aoa (degrees)',  # NASA baseline features\n",
        "    'log_dynamic_pressure', 'log_pressure',\n",
        "    'X', 'Y', 'Z', 'distance_from_center',\n",
        "    'x_normalized', 'y_normalized', 'z_normalized',\n",
        "    'velocity_cubed', 'rho_v_squared'\n",
        "]\n",
        "\n",
        "print(f\"Complete enhanced features ({len(complete_enhanced_features)}): {complete_enhanced_features}\")\n",
        "\n",
        "# Re-prepare data with complete feature set\n",
        "df_clean = df_clean.replace([np.inf, -np.inf], np.nan).dropna(subset=complete_enhanced_features + [target_variable])\n",
        "print(f\"After removing invalid values: {len(df_clean):,} points\")\n",
        "\n",
        "# Re-create feature matrix with complete features\n",
        "X_complete = df_clean[complete_enhanced_features].values\n",
        "y_complete = df_clean[target_variable].values\n",
        "\n",
        "print(f\"Complete feature matrix shape: {X_complete.shape}\")\n",
        "print(f\"Feature expansion: 6 original → {X_complete.shape[1]} enhanced features\")\n",
        "\n",
        "# Re-split data with complete features\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_complete, y_complete, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(f\"\\nFinal data splits with complete features:\")\n",
        "print(f\"Training: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
        "print(f\"Validation: {X_val.shape[0]:,} samples\")\n",
        "print(f\"Test: {X_test.shape[0]:,} samples\")\n",
        "print(\"✅ Complete feature set ready for improved ML models!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

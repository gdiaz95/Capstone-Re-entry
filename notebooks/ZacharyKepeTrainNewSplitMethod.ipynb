{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMYi0o-OffVp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "6Tv-I2kajhqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "file_path = \"/content/drive/MyDrive/NASA/apollo_cfd_database.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "NbM_-LgdjmPG",
        "outputId": "38188ec1-f8e8-45b8-a247-5ff7e9e4a921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-227806585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/NASA/apollo_cfd_database.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(file_path)\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "L59nF9V3jnSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n=== DATA CLEANING ===\")\n",
        "print(f\"Original dataset size: {len(df):,} points\")\n",
        "\n",
        "# Check for problematic values\n",
        "negative_theta = df['theta (m)'] < 0\n",
        "near_zero_re_theta = df['Re-theta'] < 1e-5\n",
        "print(f\"\\nProblematic values found:\")\n",
        "print(f\"Negative theta values: {negative_theta.sum():,}\")\n",
        "print(f\"Near-zero Re-theta values (<1e-5): {near_zero_re_theta.sum():,}\")\n",
        "\n",
        "# Create cleaned dataset\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Remove negative theta values (physically impossible)\n",
        "df_clean = df_clean[df_clean['theta (m)'] >= 0]\n",
        "print(f\"\\nAfter removing negative theta: {len(df_clean):,} points\")\n",
        "\n",
        "# Remove extremely small Re-theta values (likely numerical errors)\n",
        "df_clean = df_clean[df_clean['Re-theta'] >= 1e-5]\n",
        "print(f\"After removing near-zero Re-theta: {len(df_clean):,} points\")\n",
        "\n",
        "# Summary\n",
        "removed_points = len(df) - len(df_clean)\n",
        "percent_removed = (removed_points / len(df)) * 100\n",
        "print(f\"\\nTotal points removed: {removed_points:,} ({percent_removed:.2f}%)\")\n",
        "print(f\"Clean dataset size: {len(df_clean):,} points\")"
      ],
      "metadata": {
        "id": "cvseOiHrjp1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n=== FEATURE ENGINEERING ===\")\n",
        "\n",
        "# NASA's proven baseline features\n",
        "df_clean['log_density'] = np.log10(df_clean['density (kg/m^3)'])\n",
        "df_clean['log_velocity'] = np.log10(df_clean['velocity (m/s)'])\n",
        "\n",
        "# Physics-based features\n",
        "df_clean['log_dynamic_pressure'] = np.log10(df_clean['dynamic_pressure (Pa)'])\n",
        "df_clean['velocity_cubed'] = df_clean['velocity (m/s)'] ** 3\n",
        "df_clean['rho_v_squared'] = df_clean['density (kg/m^3)'] * (df_clean['velocity (m/s)'] ** 2)\n",
        "df_clean['velocity_squared'] = df_clean['velocity (m/s)'] ** 2\n",
        "\n",
        "# Spatial features\n",
        "df_clean['distance_from_center'] = np.sqrt(df_clean['X']**2 + df_clean['Y']**2 + df_clean['Z']**2)\n",
        "\n",
        "# Normalized spatial coordinates\n",
        "max_distance = df_clean['distance_from_center'].max()\n",
        "df_clean['x_normalized'] = df_clean['X'] / max_distance\n",
        "df_clean['y_normalized'] = df_clean['Y'] / max_distance\n",
        "df_clean['z_normalized'] = df_clean['Z'] / max_distance\n",
        "\n",
        "# Input features (14 features)\n",
        "input_features = [\n",
        "    'log_density', 'log_velocity', 'aoa (degrees)', 'log_dynamic_pressure',\n",
        "    'X', 'Y', 'Z', 'distance_from_center', 'x_normalized', 'y_normalized',\n",
        "    'z_normalized', 'velocity_cubed', 'rho_v_squared', 'velocity_squared'\n",
        "]\n",
        "\n",
        "target_variable = 'qw (W/m^2)'\n",
        "\n",
        "print(f\"Input features ({len(input_features)}):\")\n",
        "for i, feature in enumerate(input_features, 1):\n",
        "    print(f\"  {i:2d}. {feature}\")\n",
        "print(f\"\\nTarget variable: {target_variable}\")"
      ],
      "metadata": {
        "id": "X2nUcOhgjrfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Column names\n",
        "VEL = 'velocity (m/s)'\n",
        "RHO = 'density (kg/m^3)'\n",
        "AOA = 'aoa (degrees)'\n",
        "\n",
        "# Unique trajectory states\n",
        "states = df_clean[[VEL, RHO, AOA]].drop_duplicates().reset_index(drop=True)\n",
        "print(f\"Total unique trajectory states: {len(states)}\")\n",
        "\n",
        "# Step 1: Pick TEST as strictly \"in-between\" states per AoA\n",
        "test_keys = set()\n",
        "neighbor_lock = set()  # Neighbors of test states that must remain TRAIN\n",
        "\n",
        "for aoa, grp in states.groupby(AOA, sort=False):\n",
        "    # Physical ordering: velocity ↓ (fast→slow), break ties by density ↑ (thin→thick)\n",
        "    g = grp.sort_values([VEL, RHO], ascending=[False, True]).reset_index(drop=True)\n",
        "    n = len(g)\n",
        "    if n <= 2:\n",
        "        continue\n",
        "\n",
        "    mid_idx = np.arange(1, n-1)  # Strictly in-between (no endpoints)\n",
        "    # Choose ~10% of mids for test (at least 1 if possible), spaced along the sequence\n",
        "    k = max(1, int(round(0.10 * len(mid_idx))))\n",
        "    pick_positions = np.linspace(0, len(mid_idx)-1, k, dtype=int)\n",
        "    picked = mid_idx[pick_positions]\n",
        "\n",
        "    for i in picked:\n",
        "        key = tuple(g.loc[i, [VEL, RHO, AOA]])\n",
        "        left = tuple(g.loc[i-1, [VEL, RHO, AOA]])\n",
        "        right = tuple(g.loc[i+1, [VEL, RHO, AOA]])\n",
        "        test_keys.add(key)\n",
        "        neighbor_lock.add(left)\n",
        "        neighbor_lock.add(right)\n",
        "\n",
        "# Step 2: VAL from remaining states (never from neighbor-locked)\n",
        "remaining = states[~states.apply(lambda r: tuple(r) in test_keys, axis=1)]\n",
        "remain_locked = remaining[remaining.apply(lambda r: tuple(r) in neighbor_lock, axis=1)]\n",
        "remain_free = remaining[~remaining.apply(lambda r: tuple(r) in neighbor_lock, axis=1)]\n",
        "\n",
        "target_val = int(round(0.10 * len(states)))  # ≈10% of all states\n",
        "\n",
        "# Proportional per-AoA sampling for VAL from the free pool\n",
        "if len(remain_free) <= target_val:\n",
        "    val_states = remain_free\n",
        "else:\n",
        "    parts = []\n",
        "    for aoa, grp in remain_free.groupby(AOA, sort=False):\n",
        "        take = int(round(target_val * len(grp) / len(remain_free)))\n",
        "        take = min(take, len(grp))\n",
        "        if take > 0:\n",
        "            parts.append(grp.sample(n=take, random_state=42))\n",
        "    val_states = pd.concat(parts).drop_duplicates()\n",
        "    # Trim or top up slight rounding\n",
        "    if len(val_states) > target_val:\n",
        "        val_states = val_states.sample(n=target_val, random_state=42)\n",
        "\n",
        "train_states = pd.concat([\n",
        "    remain_locked,\n",
        "    remain_free[~remain_free.index.isin(val_states.index)]\n",
        "], axis=0).drop_duplicates()\n",
        "\n",
        "# Step 3: Map back to full dataframe\n",
        "train_keys = set(map(tuple, train_states[[VEL, RHO, AOA]].to_numpy()))\n",
        "val_keys = set(map(tuple, val_states[[VEL, RHO, AOA]].to_numpy()))\n",
        "\n",
        "def assign_split(row):\n",
        "    key = (row[VEL], row[RHO], row[AOA])\n",
        "    if key in test_keys:\n",
        "        return 'test'\n",
        "    if key in val_keys:\n",
        "        return 'val'\n",
        "    if key in train_keys:\n",
        "        return 'train'\n",
        "    return 'other'\n",
        "\n",
        "df_clean['split'] = df_clean.apply(assign_split, axis=1)\n",
        "\n",
        "# Step 4: Sanity checks\n",
        "uniq_counts = (df_clean[[VEL, RHO, AOA, 'split']].drop_duplicates()\n",
        "               .groupby('split').size())\n",
        "print(\"\\nUnique states per split:\")\n",
        "print(uniq_counts)\n",
        "\n",
        "# Verify \"in-between\" and neighbor-in-train per AoA\n",
        "violations = []\n",
        "for aoa, grp in states.groupby(AOA, sort=False):\n",
        "    g = grp.sort_values([VEL, RHO], ascending=[False, True]).reset_index(drop=True)\n",
        "    for i in range(1, len(g)-1):\n",
        "        key = tuple(g.loc[i, [VEL, RHO, AOA]])\n",
        "        if key in test_keys:\n",
        "            left = tuple(g.loc[i-1, [VEL, RHO, AOA]])\n",
        "            right = tuple(g.loc[i+1, [VEL, RHO, AOA]])\n",
        "            if left not in train_keys or right not in train_keys:\n",
        "                violations.append((aoa, i, left, key, right))\n",
        "\n",
        "print(f\"In-between violations: {len(violations)}\")\n",
        "\n",
        "# Extract train/val/test data\n",
        "train_df = df_clean[df_clean['split'] == 'train']\n",
        "val_df = df_clean[df_clean['split'] == 'val']\n",
        "test_df = df_clean[df_clean['split'] == 'test']\n",
        "\n",
        "X_train = train_df[input_features]\n",
        "y_train = train_df[target_variable]\n",
        "X_val = val_df[input_features]\n",
        "y_val = val_df[target_variable]\n",
        "X_test = test_df[input_features]\n",
        "y_test = test_df[target_variable]\n",
        "\n",
        "print(f\"\\nData splits:\")\n",
        "print(f\"Training: {len(X_train):,} points ({len(X_train)/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"Validation: {len(X_val):,} points ({len(X_val)/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"Test: {len(X_test):,} points ({len(X_test)/len(df_clean)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "1TIIUz81juoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use RobustScaler (less sensitive to outliers)\n",
        "scaler_X = RobustScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_val_scaled = scaler_X.transform(X_val)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "# Log transform and robust scale targets\n",
        "y_train_log = np.log10(y_train)\n",
        "y_val_log = np.log10(y_val)\n",
        "y_test_log = np.log10(y_test)\n",
        "\n",
        "scaler_y = RobustScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_log.values.reshape(-1, 1)).flatten()\n",
        "y_val_scaled = scaler_y.transform(y_val_log.values.reshape(-1, 1)).flatten()\n",
        "y_test_scaled = scaler_y.transform(y_test_log.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "print(f\"RobustScaler applied to {X_train_scaled.shape[1]} input features\")\n",
        "print(f\"Target scaling - Log range: {y_train_log.min():.3f} to {y_train_log.max():.3f}\")\n",
        "print(\"Data scaling complete.\")"
      ],
      "metadata": {
        "id": "-vNk8uopjwqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_improved_model(input_shape):\n",
        "    \"\"\"Create improved neural network architecture for heat flux prediction\"\"\"\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_shape,)),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(16, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0001)),\n",
        "        layers.Dense(1, activation='linear')  # Linear activation for regression\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "model = create_improved_model(X_train_scaled.shape[1])\n",
        "\n",
        "# Configure optimizer\n",
        "optimizer = keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    beta_1=0.9,\n",
        "    beta_2=0.999,\n",
        "    epsilon=1e-7\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='huber',  # More robust to outliers than MSE\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(f\"Model architecture: {len(input_features)}→256→128→64→32→16→1\")\n",
        "print(f\"Total parameters: {model.count_params():,}\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "4W-3ZQvsjzLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.7,\n",
        "        patience=10,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        'best_heat_flux_model.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"Starting training...\")\n",
        "history = model.fit(\n",
        "    X_train_scaled, y_train_scaled,\n",
        "    validation_data=(X_val_scaled, y_val_scaled),\n",
        "    epochs=150,\n",
        "    batch_size=2048,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "kaemml5wj09O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_actual, y_predicted_scaled, scaler_y, model_name=\"Model\"):\n",
        "    \"\"\"Comprehensive model evaluation function\"\"\"\n",
        "    # Inverse transform predictions\n",
        "    y_predicted_log = scaler_y.inverse_transform(y_predicted_scaled.reshape(-1, 1)).flatten()\n",
        "    y_predicted = 10**y_predicted_log  # Inverse log10 transform\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_actual, y_predicted)\n",
        "    mse = mean_squared_error(y_actual, y_predicted)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # Calculate relative error for each point\n",
        "    relative_errors = np.abs(y_actual - y_predicted) / y_actual * 100\n",
        "\n",
        "    # Percentage within ±5% error (NASA target)\n",
        "    percent_within_5 = np.sum(relative_errors <= 5) / len(relative_errors) * 100\n",
        "\n",
        "    # Median relative error\n",
        "    median_error = np.median(relative_errors)\n",
        "\n",
        "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
        "    print(f\"  MAE: {mae:.2f} W/m²\")\n",
        "    print(f\"  MSE: {mse:.2f}\")\n",
        "    print(f\"  RMSE: {rmse:.2f} W/m²\")\n",
        "    print(f\"  Percentage within ±5% error: {percent_within_5:.2f}%\")\n",
        "    print(f\"  Median Relative Error: {median_error:.2f}%\")\n",
        "    print(\"------------------------------\")\n",
        "\n",
        "    return {\n",
        "        'mae': mae,\n",
        "        'mse': mse,\n",
        "        'rmse': rmse,\n",
        "        'percent_within_5': percent_within_5,\n",
        "        'median_error': median_error,\n",
        "        'relative_errors': relative_errors,\n",
        "        'predictions': y_predicted\n",
        "    }\n",
        "\n",
        "# Make predictions and evaluate\n",
        "print(\"Making predictions on test set...\")\n",
        "y_pred_scaled = model.predict(X_test_scaled, verbose=0)\n",
        "results = evaluate_model(y_test, y_pred_scaled, scaler_y, \"Neural Network\")"
      ],
      "metadata": {
        "id": "Y4kDrLFpj54i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "fig.suptitle('NASA Heat Flux Prediction: Neural Network Results', fontsize=16)\n",
        "\n",
        "# 1. NASA target comparison\n",
        "nasa_target = 95.0\n",
        "current_accuracy = results['percent_within_5']\n",
        "\n",
        "models = ['Current Model', 'NASA Target']\n",
        "accuracies = [current_accuracy, nasa_target]\n",
        "colors = ['green' if current_accuracy >= nasa_target else 'orange', 'blue']\n",
        "\n",
        "bars = axes[0,0].bar(models, accuracies, color=colors, alpha=0.7)\n",
        "axes[0,0].set_ylabel('% Within ±5%')\n",
        "axes[0,0].set_title('Performance vs NASA Target')\n",
        "axes[0,0].set_ylim(0, 100)\n",
        "\n",
        "# Add value labels\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                   f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 2. Predicted vs Actual (log scale)\n",
        "axes[0,1].scatter(y_test, results['predictions'], alpha=0.1, s=0.5, color='purple')\n",
        "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', alpha=0.8)\n",
        "axes[0,1].set_xlabel('Actual Heat Flux (W/m²)')\n",
        "axes[0,1].set_ylabel('Predicted Heat Flux (W/m²)')\n",
        "axes[0,1].set_title('Predicted vs Actual Heat Flux')\n",
        "axes[0,1].set_xscale('log')\n",
        "axes[0,1].set_yscale('log')\n",
        "\n",
        "# 3. Relative error distribution\n",
        "axes[0,2].hist(results['relative_errors'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[0,2].axvline(5, color='red', linestyle='--', label='NASA ±5% Target')\n",
        "axes[0,2].axvline(results['median_error'], color='green', linestyle='-',\n",
        "                  label=f'Median: {results[\"median_error\"]:.1f}%')\n",
        "axes[0,2].set_xlabel('Relative Error (%)')\n",
        "axes[0,2].set_ylabel('Frequency')\n",
        "axes[0,2].set_title('Relative Error Distribution')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].set_xlim(0, 25)\n",
        "\n",
        "# 4. Training history\n",
        "axes[1,0].plot(history.history['loss'], label='Training Loss', alpha=0.8)\n",
        "axes[1,0].plot(history.history['val_loss'], label='Validation Loss', alpha=0.8)\n",
        "axes[1,0].set_xlabel('Epoch')\n",
        "axes[1,0].set_ylabel('Loss (Huber)')\n",
        "axes[1,0].set_title('Training History')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].set_yscale('log')\n",
        "\n",
        "# 5. Error vs heat flux magnitude\n",
        "axes[1,1].scatter(y_test, results['relative_errors'], alpha=0.1, s=0.5, color='purple')\n",
        "axes[1,1].axhline(5, color='red', linestyle='--', label='NASA ±5% Target')\n",
        "axes[1,1].set_xlabel('Actual Heat Flux (W/m²)')\n",
        "axes[1,1].set_ylabel('Relative Error (%)')\n",
        "axes[1,1].set_title('Error vs Heat Flux Magnitude')\n",
        "axes[1,1].set_xscale('log')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].set_ylim(0, 50)\n",
        "\n",
        "# 6. Error breakdown pie chart\n",
        "within_5 = results['percent_within_5']\n",
        "within_5_10 = np.sum((results['relative_errors'] > 5) & (results['relative_errors'] <= 10)) / len(results['relative_errors']) * 100\n",
        "above_10 = 100 - within_5 - within_5_10\n",
        "\n",
        "labels = ['Within ±5%\\n(NASA Target)', 'Within ±5-10%', 'Above ±10%']\n",
        "sizes = [within_5, within_5_10, above_10]\n",
        "colors_pie = ['green', 'yellow', 'red']\n",
        "explode = (0.1, 0, 0)\n",
        "\n",
        "axes[1,2].pie(sizes, explode=explode, labels=labels, colors=colors_pie,\n",
        "              autopct='%1.1f%%', shadow=True)\n",
        "axes[1,2].set_title('Error Distribution Breakdown')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LvdsDGjJj-s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nMODEL CONFIGURATION:\")\n",
        "print(f\"   Architecture: {len(input_features)}→256→128→64→32→16→1\")\n",
        "print(f\"   Parameters: {model.count_params():,}\")\n",
        "print(f\"   Training epochs: {len(history.history['loss'])} (early stopped)\")\n",
        "\n",
        "print(f\"\\nPERFORMANCE RESULTS:\")\n",
        "print(f\"   Current model: {results['percent_within_5']:.1f}% within ±5%\")\n",
        "print(f\"   NASA target: 95.0% within ±5%\")\n",
        "print(f\"   Median error: {results['median_error']:.1f}%\")\n",
        "\n",
        "if results['percent_within_5'] >= 95:\n",
        "    print(f\"   SUCCESS: Beat NASA's interpolation baseline!\")\n",
        "else:\n",
        "    remaining = 95 - results['percent_within_5']\n",
        "    print(f\"   Progress: {remaining:.1f}% improvement needed to reach NASA baseline\")\n",
        "\n",
        "print(f\"\\nAnalysis complete!\")"
      ],
      "metadata": {
        "id": "jbksIfztkAhI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
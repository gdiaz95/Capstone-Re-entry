{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NASA Apollo CFD - Autoencoder Approach for Heat Flux Prediction\n",
        "\n",
        "This notebook implements an autoencoder-based approach to predict heat flux (qw) from CFD data.\n",
        "\n",
        "**Approach:**\n",
        "- **Variational Autoencoder (VAE)** to learn compressed latent representations\n",
        "- **Encoder**: Compresses input features to latent space\n",
        "- **Decoder**: Reconstructs features from latent space\n",
        "- **Predictor**: Uses latent representation to predict heat flux\n",
        "- **Benefits**: Denoising, regularization, feature learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"=== NASA APOLLO CFD - AUTOENCODER MODEL ===\\n\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = \"/content/drive/MyDrive/NASA/apollo_cfd_database.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(f\"Dataset shape: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== DATA CLEANING ===\")\n",
        "print(f\"Original: {len(df):,} points\")\n",
        "\n",
        "df_clean = df.copy()\n",
        "df_clean = df_clean[df_clean['theta (m)'] >= 0]\n",
        "df_clean = df_clean[df_clean['Re-theta'] >= 1e-5]\n",
        "df_clean = df_clean[(df_clean['qw (W/m^2)'] >= 1e3) & (df_clean['qw (W/m^2)'] <= 1e7)]\n",
        "\n",
        "print(f\"Removed: {len(df) - len(df_clean):,} points\")\n",
        "print(f\"Clean: {len(df_clean):,} points\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== FEATURE ENGINEERING ===\")\n",
        "\n",
        "# NASA baseline\n",
        "df_clean['log_density'] = np.log10(df_clean['density (kg/m^3)'])\n",
        "df_clean['log_velocity'] = np.log10(df_clean['velocity (m/s)'])\n",
        "\n",
        "# Reynolds numbers\n",
        "MU_REF = 1.8e-5\n",
        "df_clean['char_length'] = np.sqrt(df_clean['X']**2 + df_clean['Y']**2 + df_clean['Z']**2)\n",
        "df_clean['reynolds'] = (df_clean['density (kg/m^3)'] * df_clean['velocity (m/s)'] * \n",
        "                         df_clean['char_length']) / MU_REF\n",
        "df_clean['log_reynolds'] = np.log10(df_clean['reynolds'] + 1e-10)\n",
        "df_clean['sqrt_reynolds'] = np.sqrt(df_clean['reynolds'])\n",
        "\n",
        "df_clean['re_theta'] = (df_clean['density (kg/m^3)'] * df_clean['velocity (m/s)'] * \n",
        "                         df_clean['theta (m)']) / MU_REF\n",
        "df_clean['log_re_theta'] = np.log10(df_clean['re_theta'] + 1e-10)\n",
        "\n",
        "# Mach\n",
        "df_clean['mach_sq'] = df_clean['mach (-)'] ** 2\n",
        "df_clean['log_mach'] = np.log10(df_clean['mach (-)'] + 1e-10)\n",
        "df_clean['log_me'] = np.log10(df_clean['Me'] + 1e-10)\n",
        "\n",
        "# Boundary layer\n",
        "df_clean['log_delta'] = np.log10(df_clean['delta (m)'] + 1e-10)\n",
        "df_clean['log_theta'] = np.log10(df_clean['theta (m)'] + 1e-10)\n",
        "\n",
        "# Stagnation\n",
        "df_clean['is_stagnation'] = (df_clean['char_length'] < 0.05).astype(np.float32)\n",
        "df_clean['radial_dist'] = np.sqrt(df_clean['X']**2 + df_clean['Y']**2)\n",
        "\n",
        "# Sutton-Graves\n",
        "df_clean['sutton_graves'] = (np.sqrt(df_clean['density (kg/m^3)']) * \n",
        "                              (df_clean['velocity (m/s)'] ** 3)) / (df_clean['char_length'] + 1e-10)\n",
        "df_clean['log_sutton_graves'] = np.log10(df_clean['sutton_graves'] + 1e-10)\n",
        "\n",
        "# Angle\n",
        "df_clean['cos_aoa'] = np.cos(np.radians(df_clean['aoa (degrees)']))\n",
        "df_clean['sin_aoa'] = np.sin(np.radians(df_clean['aoa (degrees)']))\n",
        "df_clean['log_dyn_pressure'] = np.log10(df_clean['dynamic_pressure (Pa)'])\n",
        "\n",
        "input_features = [\n",
        "    'log_density', 'log_velocity', 'aoa (degrees)',\n",
        "    'log_reynolds', 'sqrt_reynolds', 'log_re_theta',\n",
        "    'mach_sq', 'log_mach', 'log_me',\n",
        "    'sutton_graves', 'log_sutton_graves',\n",
        "    'log_dyn_pressure', 'log_delta', 'log_theta',\n",
        "    'is_stagnation', 'radial_dist',\n",
        "    'cos_aoa', 'sin_aoa'\n",
        "]\n",
        "\n",
        "target_variable = 'qw (W/m^2)'\n",
        "print(f\"Features: {len(input_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== TRAJECTORY STATE SPLIT ===\")\n",
        "\n",
        "VEL = 'velocity (m/s)'\n",
        "RHO = 'density (kg/m^3)'\n",
        "AOA = 'aoa (degrees)'\n",
        "\n",
        "states = df_clean[[VEL, RHO, AOA]].drop_duplicates().reset_index(drop=True)\n",
        "print(f\"Unique states: {len(states)}\")\n",
        "\n",
        "# Velocity bins for stratification\n",
        "velocity_bins = pd.qcut(df_clean[VEL], q=5, labels=['v1','v2','v3','v4','v5'], duplicates='drop')\n",
        "df_clean['vel_bin'] = velocity_bins\n",
        "\n",
        "test_keys = set()\n",
        "neighbor_lock = set()\n",
        "\n",
        "for aoa, grp in states.groupby(AOA, sort=False):\n",
        "    g = grp.sort_values([VEL, RHO], ascending=[False, True]).reset_index(drop=True)\n",
        "    n = len(g)\n",
        "    if n <= 2:\n",
        "        continue\n",
        "    \n",
        "    mid_idx = np.arange(1, n-1)\n",
        "    k = max(1, int(round(0.10 * len(mid_idx))))\n",
        "    pick_pos = np.linspace(0, len(mid_idx)-1, k, dtype=int)\n",
        "    \n",
        "    for i in mid_idx[pick_pos]:\n",
        "        key = tuple(g.loc[i, [VEL, RHO, AOA]])\n",
        "        left = tuple(g.loc[i-1, [VEL, RHO, AOA]])\n",
        "        right = tuple(g.loc[i+1, [VEL, RHO, AOA]])\n",
        "        test_keys.add(key)\n",
        "        neighbor_lock.add(left)\n",
        "        neighbor_lock.add(right)\n",
        "\n",
        "remaining = states[~states.apply(lambda r: tuple(r) in test_keys, axis=1)]\n",
        "remain_locked = remaining[remaining.apply(lambda r: tuple(r) in neighbor_lock, axis=1)]\n",
        "remain_free = remaining[~remaining.apply(lambda r: tuple(r) in neighbor_lock, axis=1)]\n",
        "\n",
        "target_val = int(round(0.10 * len(states)))\n",
        "\n",
        "if len(remain_free) <= target_val:\n",
        "    val_states = remain_free\n",
        "else:\n",
        "    parts = []\n",
        "    for aoa, grp in remain_free.groupby(AOA, sort=False):\n",
        "        take = int(round(target_val * len(grp) / len(remain_free)))\n",
        "        take = min(take, len(grp))\n",
        "        if take > 0:\n",
        "            parts.append(grp.sample(n=take, random_state=42))\n",
        "    val_states = pd.concat(parts).drop_duplicates()\n",
        "    if len(val_states) > target_val:\n",
        "        val_states = val_states.sample(n=target_val, random_state=42)\n",
        "\n",
        "train_states = pd.concat([remain_locked, remain_free[~remain_free.index.isin(val_states.index)]]).drop_duplicates()\n",
        "\n",
        "train_keys = set(map(tuple, train_states[[VEL, RHO, AOA]].to_numpy()))\n",
        "val_keys = set(map(tuple, val_states[[VEL, RHO, AOA]].to_numpy()))\n",
        "\n",
        "# VECTORIZED assignment\n",
        "print(\"Assigning splits...\")\n",
        "df_clean['state_key'] = list(zip(df_clean[VEL], df_clean[RHO], df_clean[AOA]))\n",
        "\n",
        "df_clean['split'] = 'other'\n",
        "df_clean.loc[df_clean['state_key'].isin(test_keys), 'split'] = 'test'\n",
        "df_clean.loc[df_clean['state_key'].isin(val_keys), 'split'] = 'val'\n",
        "df_clean.loc[df_clean['state_key'].isin(train_keys), 'split'] = 'train'\n",
        "\n",
        "df_clean.drop('state_key', axis=1, inplace=True)\n",
        "\n",
        "print(df_clean['split'].value_counts())\n",
        "\n",
        "train_df = df_clean[df_clean['split'] == 'train']\n",
        "val_df = df_clean[df_clean['split'] == 'val']\n",
        "test_df = df_clean[df_clean['split'] == 'test']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== DATA PREPARATION ===\")\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# Using full dataset (no downsampling)\n",
        "print(f\"Train: {len(train_df):,}, Val: {len(val_df):,}, Test: {len(test_df):,}\")\n",
        "\n",
        "# Convert to float32 and extract\n",
        "X_train = train_df[input_features].values.astype(np.float32)\n",
        "y_train = train_df[target_variable].values.astype(np.float32)\n",
        "X_val = val_df[input_features].values.astype(np.float32)\n",
        "y_val = val_df[target_variable].values.astype(np.float32)\n",
        "X_test = test_df[input_features].values.astype(np.float32)\n",
        "y_test = test_df[target_variable].values.astype(np.float32)\n",
        "\n",
        "# Scale features\n",
        "print(\"Scaling features...\")\n",
        "scaler_X = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train).astype(np.float32)\n",
        "X_val_scaled = scaler_X.transform(X_val).astype(np.float32)\n",
        "X_test_scaled = scaler_X.transform(X_test).astype(np.float32)\n",
        "\n",
        "# Scale targets\n",
        "y_train_log = np.log10(y_train).astype(np.float32)\n",
        "y_val_log = np.log10(y_val).astype(np.float32)\n",
        "y_test_log = np.log10(y_test).astype(np.float32)\n",
        "\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train_log.reshape(-1, 1)).flatten().astype(np.float32)\n",
        "y_val_scaled = scaler_y.transform(y_val_log.reshape(-1, 1)).flatten().astype(np.float32)\n",
        "y_test_scaled = scaler_y.transform(y_test_log.reshape(-1, 1)).flatten().astype(np.float32)\n",
        "\n",
        "print(f\"\u2705 Data prepared: {X_train_scaled.shape}\")\n",
        "\n",
        "del X_train, X_val, X_test, y_train_log, y_val_log\n",
        "gc.collect()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build Autoencoder Model\n",
        "\n",
        "**Architecture:**\n",
        "- **Encoder**: Compresses 18D input \u2192 latent space (16D)\n",
        "- **Decoder**: Reconstructs 18D features from latent space\n",
        "- **Predictor**: Maps latent features \u2192 heat flux prediction\n",
        "\n",
        "The autoencoder learns meaningful compressed representations that:\n",
        "1. Remove noise and redundancy\n",
        "2. Capture essential physics\n",
        "3. Regularize the feature space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== BUILDING AUTOENCODER MODEL ===\")\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = X_train_scaled.shape[1]\n",
        "latent_dim = 16  # Compressed representation\n",
        "encoder_layers = [128, 64, 32]\n",
        "decoder_layers = [32, 64, 128]\n",
        "predictor_layers = [64, 32, 16]\n",
        "\n",
        "# ============================================================================\n",
        "# ENCODER: Input \u2192 Latent Space\n",
        "# ============================================================================\n",
        "encoder_input = layers.Input(shape=(input_dim,), name='encoder_input')\n",
        "x = encoder_input\n",
        "\n",
        "for i, units in enumerate(encoder_layers):\n",
        "    x = layers.Dense(units, activation='relu', \n",
        "                     kernel_regularizer=keras.regularizers.l2(0.0001),\n",
        "                     name=f'encoder_{i+1}')(x)\n",
        "    x = layers.BatchNormalization(name=f'encoder_bn_{i+1}')(x)\n",
        "    x = layers.Dropout(0.2, name=f'encoder_dropout_{i+1}')(x)\n",
        "\n",
        "# Latent representation\n",
        "latent = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, latent, name='encoder')\n",
        "\n",
        "# ============================================================================\n",
        "# DECODER: Latent Space \u2192 Reconstructed Input\n",
        "# ============================================================================\n",
        "latent_input = layers.Input(shape=(latent_dim,), name='latent_input')\n",
        "x = latent_input\n",
        "\n",
        "for i, units in enumerate(decoder_layers):\n",
        "    x = layers.Dense(units, activation='relu',\n",
        "                     kernel_regularizer=keras.regularizers.l2(0.0001),\n",
        "                     name=f'decoder_{i+1}')(x)\n",
        "    x = layers.BatchNormalization(name=f'decoder_bn_{i+1}')(x)\n",
        "    x = layers.Dropout(0.2, name=f'decoder_dropout_{i+1}')(x)\n",
        "\n",
        "reconstructed = layers.Dense(input_dim, activation='linear', name='reconstructed')(x)\n",
        "\n",
        "decoder = keras.Model(latent_input, reconstructed, name='decoder')\n",
        "\n",
        "# ============================================================================\n",
        "# AUTOENCODER: Input \u2192 Latent \u2192 Reconstructed\n",
        "# ============================================================================\n",
        "autoencoder_input = layers.Input(shape=(input_dim,), name='autoencoder_input')\n",
        "encoded = encoder(autoencoder_input)\n",
        "decoded = decoder(encoded)\n",
        "\n",
        "autoencoder = keras.Model(autoencoder_input, decoded, name='autoencoder')\n",
        "\n",
        "# ============================================================================\n",
        "# PREDICTOR: Latent Space \u2192 Heat Flux\n",
        "# ============================================================================\n",
        "predictor_input = layers.Input(shape=(latent_dim,), name='predictor_input')\n",
        "x = predictor_input\n",
        "\n",
        "for i, units in enumerate(predictor_layers):\n",
        "    x = layers.Dense(units, activation='relu',\n",
        "                     kernel_regularizer=keras.regularizers.l2(0.0001),\n",
        "                     name=f'predictor_{i+1}')(x)\n",
        "    x = layers.BatchNormalization(name=f'predictor_bn_{i+1}')(x)\n",
        "    x = layers.Dropout(0.2, name=f'predictor_dropout_{i+1}')(x)\n",
        "\n",
        "heat_flux_pred = layers.Dense(1, activation='linear', name='heat_flux_output')(x)\n",
        "\n",
        "predictor = keras.Model(predictor_input, heat_flux_pred, name='predictor')\n",
        "\n",
        "# ============================================================================\n",
        "# FULL MODEL: Input \u2192 Latent \u2192 Heat Flux\n",
        "# ============================================================================\n",
        "full_input = layers.Input(shape=(input_dim,), name='full_input')\n",
        "full_encoded = encoder(full_input)\n",
        "full_prediction = predictor(full_encoded)\n",
        "\n",
        "full_model = keras.Model(full_input, full_prediction, name='encoder_predictor')\n",
        "\n",
        "print(\"\\n=== MODEL SUMMARIES ===\")\n",
        "print(f\"\\nEncoder: {input_dim}D \u2192 {latent_dim}D\")\n",
        "encoder.summary()\n",
        "print(f\"\\nDecoder: {latent_dim}D \u2192 {input_dim}D\")\n",
        "decoder.summary()\n",
        "print(f\"\\nPredictor: {latent_dim}D \u2192 1D (heat flux)\")\n",
        "predictor.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== PHASE 1: AUTOENCODER PRETRAINING ===\")\n",
        "\n",
        "# Compile autoencoder\n",
        "autoencoder.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "ae_callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        patience=20, \n",
        "        restore_best_weights=True, \n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', \n",
        "        factor=0.5, \n",
        "        patience=10, \n",
        "        min_lr=1e-7, \n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train autoencoder to reconstruct input features\n",
        "history_ae = autoencoder.fit(\n",
        "    X_train_scaled, X_train_scaled,  # Input = Output for autoencoder\n",
        "    validation_data=(X_val_scaled, X_val_scaled),\n",
        "    epochs=100,\n",
        "    batch_size=8192,\n",
        "    callbacks=ae_callbacks,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Autoencoder pretraining complete: {len(history_ae.history['loss'])} epochs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train Heat Flux Predictor\n",
        "\n",
        "Now train the predictor using the learned latent representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== PHASE 2: HEAT FLUX PREDICTOR TRAINING ===\")\n",
        "\n",
        "# Option 1: Freeze encoder (transfer learning)\n",
        "# encoder.trainable = False\n",
        "\n",
        "# Option 2: Fine-tune entire model (better performance)\n",
        "encoder.trainable = True\n",
        "\n",
        "# Custom weighted MSE loss\n",
        "def weighted_mse_loss(y_true, y_pred):\n",
        "    weights = 1.0 + tf.abs(y_true) / (tf.reduce_mean(tf.abs(y_true)) + 1e-7)\n",
        "    return tf.reduce_mean(weights * tf.square(y_true - y_pred))\n",
        "\n",
        "# Compile full model\n",
        "full_model.compile(\n",
        "    optimizer=keras.optimizers.AdamW(\n",
        "        learning_rate=0.001,\n",
        "        weight_decay=0.01,\n",
        "        clipnorm=1.0\n",
        "    ),\n",
        "    loss=weighted_mse_loss,\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "print(f\"Total parameters: {full_model.count_params():,}\")\n",
        "\n",
        "# Callbacks\n",
        "pred_callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        patience=30, \n",
        "        restore_best_weights=True, \n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss', \n",
        "        factor=0.5, \n",
        "        patience=15, \n",
        "        min_lr=1e-8, \n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        'best_autoencoder_model.keras', \n",
        "        monitor='val_loss', \n",
        "        save_best_only=True, \n",
        "        verbose=0\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train predictor\n",
        "history_pred = full_model.fit(\n",
        "    X_train_scaled, y_train_scaled,\n",
        "    validation_data=(X_val_scaled, y_val_scaled),\n",
        "    epochs=200,\n",
        "    batch_size=16384,\n",
        "    callbacks=pred_callbacks,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Predictor training complete: {len(history_pred.history['loss'])} epochs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== EVALUATION ===\")\n",
        "\n",
        "# Predictions\n",
        "y_pred_scaled = full_model.predict(X_test_scaled, verbose=0)\n",
        "y_pred_log = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "y_pred = 10**y_pred_log\n",
        "\n",
        "# Metrics\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "rel_errors = np.abs(y_test - y_pred) / y_test * 100\n",
        "\n",
        "pct_1 = np.sum(rel_errors <= 1) / len(rel_errors) * 100\n",
        "pct_3 = np.sum(rel_errors <= 3) / len(rel_errors) * 100\n",
        "pct_5 = np.sum(rel_errors <= 5) / len(rel_errors) * 100\n",
        "pct_10 = np.sum(rel_errors <= 10) / len(rel_errors) * 100\n",
        "\n",
        "median_err = np.median(rel_errors)\n",
        "q95_err = np.percentile(rel_errors, 95)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"AUTOENCODER MODEL PERFORMANCE\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"MAE: {mae:.0f} W/m\u00b2\")\n",
        "print(f\"RMSE: {rmse:.0f} W/m\u00b2\")\n",
        "print(f\"Within \u00b11%: {pct_1:.1f}%\")\n",
        "print(f\"Within \u00b13%: {pct_3:.1f}%\")\n",
        "print(f\"Within \u00b15%: {pct_5:.1f}% \u2b50 (NASA Target: 95%)\")\n",
        "print(f\"Within \u00b110%: {pct_10:.1f}%\")\n",
        "print(f\"Median error: {median_err:.2f}%\")\n",
        "print(f\"95th %ile: {q95_err:.1f}%\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Latent Space Analysis\n",
        "\n",
        "Visualize the learned latent representations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== LATENT SPACE ANALYSIS ===\")\n",
        "\n",
        "# Sample data for visualization (too large otherwise)\n",
        "sample_size = 10000\n",
        "sample_idx = np.random.choice(len(X_test_scaled), sample_size, replace=False)\n",
        "X_sample = X_test_scaled[sample_idx]\n",
        "y_sample = y_test[sample_idx]\n",
        "\n",
        "# Get latent representations\n",
        "latent_repr = encoder.predict(X_sample, verbose=0)\n",
        "\n",
        "print(f\"Latent representation shape: {latent_repr.shape}\")\n",
        "print(f\"Latent mean: {latent_repr.mean():.4f}\")\n",
        "print(f\"Latent std: {latent_repr.std():.4f}\")\n",
        "\n",
        "# PCA for 2D visualization\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "latent_2d = pca.fit_transform(latent_repr)\n",
        "\n",
        "print(f\"PCA variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n=== VISUALIZATIONS ===\")\n",
        "\n",
        "fig = plt.figure(figsize=(24, 16))\n",
        "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
        "fig.suptitle('NASA Heat Flux: Autoencoder Model', fontsize=18, fontweight='bold')\n",
        "\n",
        "# ============================================================================\n",
        "# Row 1: Model Performance\n",
        "# ============================================================================\n",
        "\n",
        "# 1.1 Performance vs NASA\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "bars = ax1.bar(['Autoencoder', 'NASA Target'], [pct_5, 95.0],\n",
        "               color=['green' if pct_5>=95 else 'orange', 'blue'], alpha=0.7)\n",
        "ax1.set_ylabel('% Within \u00b15%', fontsize=12)\n",
        "ax1.set_title('Performance vs NASA Target', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylim(0, 100)\n",
        "for bar, val in zip(bars, [pct_5, 95.0]):\n",
        "    ax1.text(bar.get_x()+bar.get_width()/2, bar.get_height()+1,\n",
        "            f'{val:.1f}%', ha='center', fontweight='bold', fontsize=11)\n",
        "\n",
        "# 1.2 Predicted vs Actual\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "scatter = ax2.scatter(y_test, y_pred, alpha=0.1, s=1, c=rel_errors, \n",
        "                      cmap='RdYlGn_r', vmin=0, vmax=10)\n",
        "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "ax2.set_xlabel('Actual Heat Flux (W/m\u00b2)', fontsize=12)\n",
        "ax2.set_ylabel('Predicted Heat Flux (W/m\u00b2)', fontsize=12)\n",
        "ax2.set_title('Predicted vs Actual', fontsize=12, fontweight='bold')\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_yscale('log')\n",
        "plt.colorbar(scatter, ax=ax2, label='Error (%)')\n",
        "\n",
        "# 1.3 Error Distribution\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "ax3.hist(rel_errors, bins=100, alpha=0.7, color='purple', edgecolor='black')\n",
        "ax3.axvline(5, color='red', linestyle='--', linewidth=2, label='NASA \u00b15%')\n",
        "ax3.axvline(median_err, color='green', linestyle='-', linewidth=2, \n",
        "            label=f'Median: {median_err:.1f}%')\n",
        "ax3.set_xlabel('Relative Error (%)', fontsize=12)\n",
        "ax3.set_ylabel('Frequency', fontsize=12)\n",
        "ax3.set_title('Error Distribution', fontsize=12, fontweight='bold')\n",
        "ax3.legend(fontsize=10)\n",
        "ax3.set_xlim(0, 20)\n",
        "\n",
        "# 1.4 Error Breakdown (Pie)\n",
        "ax4 = fig.add_subplot(gs[0, 3])\n",
        "w1 = pct_1\n",
        "w3 = pct_3 - pct_1\n",
        "w5 = pct_5 - pct_3\n",
        "w10 = pct_10 - pct_5\n",
        "above = 100 - pct_10\n",
        "ax4.pie([w1, w3, w5, w10, above], labels=['\u00b11%', '1-3%', '3-5%', '5-10%', '>10%'],\n",
        "        colors=['darkgreen', 'lightgreen', 'yellow', 'orange', 'red'],\n",
        "        autopct='%1.1f%%', startangle=90, textprops={'fontsize': 10})\n",
        "ax4.set_title('Error Breakdown', fontsize=12, fontweight='bold')\n",
        "\n",
        "# ============================================================================\n",
        "# Row 2: Training History\n",
        "# ============================================================================\n",
        "\n",
        "# 2.1 Autoencoder Training\n",
        "ax5 = fig.add_subplot(gs[1, 0])\n",
        "ax5.plot(history_ae.history['loss'], label='Train', alpha=0.8, linewidth=2)\n",
        "ax5.plot(history_ae.history['val_loss'], label='Val', alpha=0.8, linewidth=2)\n",
        "ax5.set_xlabel('Epoch', fontsize=12)\n",
        "ax5.set_ylabel('Reconstruction Loss', fontsize=12)\n",
        "ax5.set_title('Autoencoder Pretraining', fontsize=12, fontweight='bold')\n",
        "ax5.legend(fontsize=10)\n",
        "ax5.set_yscale('log')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 2.2 Predictor Training\n",
        "ax6 = fig.add_subplot(gs[1, 1])\n",
        "ax6.plot(history_pred.history['loss'], label='Train', alpha=0.8, linewidth=2)\n",
        "ax6.plot(history_pred.history['val_loss'], label='Val', alpha=0.8, linewidth=2)\n",
        "ax6.set_xlabel('Epoch', fontsize=12)\n",
        "ax6.set_ylabel('Prediction Loss', fontsize=12)\n",
        "ax6.set_title('Heat Flux Predictor Training', fontsize=12, fontweight='bold')\n",
        "ax6.legend(fontsize=10)\n",
        "ax6.set_yscale('log')\n",
        "ax6.grid(True, alpha=0.3)\n",
        "\n",
        "# 2.3 Error vs Magnitude\n",
        "ax7 = fig.add_subplot(gs[1, 2])\n",
        "ax7.scatter(y_test, rel_errors, alpha=0.1, s=1, color='purple')\n",
        "ax7.axhline(5, color='red', linestyle='--', linewidth=2, label='NASA \u00b15%')\n",
        "ax7.set_xlabel('Actual Heat Flux (W/m\u00b2)', fontsize=12)\n",
        "ax7.set_ylabel('Relative Error (%)', fontsize=12)\n",
        "ax7.set_title('Error vs Heat Flux Magnitude', fontsize=12, fontweight='bold')\n",
        "ax7.set_xscale('log')\n",
        "ax7.set_ylim(0, 30)\n",
        "ax7.legend(fontsize=10)\n",
        "ax7.grid(True, alpha=0.3)\n",
        "\n",
        "# 2.4 Reconstruction Quality\n",
        "ax8 = fig.add_subplot(gs[1, 3])\n",
        "X_reconstructed = autoencoder.predict(X_sample, verbose=0)\n",
        "reconstruction_errors = np.mean(np.abs(X_sample - X_reconstructed), axis=1)\n",
        "ax8.hist(reconstruction_errors, bins=50, alpha=0.7, color='teal', edgecolor='black')\n",
        "ax8.axvline(reconstruction_errors.mean(), color='red', linestyle='--', linewidth=2,\n",
        "            label=f'Mean: {reconstruction_errors.mean():.4f}')\n",
        "ax8.set_xlabel('Reconstruction Error', fontsize=12)\n",
        "ax8.set_ylabel('Frequency', fontsize=12)\n",
        "ax8.set_title('Feature Reconstruction Quality', fontsize=12, fontweight='bold')\n",
        "ax8.legend(fontsize=10)\n",
        "\n",
        "# ============================================================================\n",
        "# Row 3: Latent Space Analysis\n",
        "# ============================================================================\n",
        "\n",
        "# 3.1 Latent Space (colored by heat flux)\n",
        "ax9 = fig.add_subplot(gs[2, 0])\n",
        "scatter = ax9.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
        "                      c=np.log10(y_sample), s=2, alpha=0.5, cmap='viridis')\n",
        "ax9.set_xlabel('Latent Dim 1', fontsize=12)\n",
        "ax9.set_ylabel('Latent Dim 2', fontsize=12)\n",
        "ax9.set_title('Latent Space (colored by log10(heat flux))', fontsize=12, fontweight='bold')\n",
        "plt.colorbar(scatter, ax=ax9, label='log10(Heat Flux)')\n",
        "\n",
        "# 3.2 Latent Space (colored by error)\n",
        "ax10 = fig.add_subplot(gs[2, 1])\n",
        "y_pred_sample = 10**scaler_y.inverse_transform(\n",
        "    full_model.predict(X_sample, verbose=0).reshape(-1, 1)\n",
        ").flatten()\n",
        "errors_sample = np.abs(y_sample - y_pred_sample) / y_sample * 100\n",
        "scatter = ax10.scatter(latent_2d[:, 0], latent_2d[:, 1], \n",
        "                       c=errors_sample, s=2, alpha=0.5, cmap='RdYlGn_r', vmin=0, vmax=10)\n",
        "ax10.set_xlabel('Latent Dim 1', fontsize=12)\n",
        "ax10.set_ylabel('Latent Dim 2', fontsize=12)\n",
        "ax10.set_title('Latent Space (colored by prediction error)', fontsize=12, fontweight='bold')\n",
        "plt.colorbar(scatter, ax=ax10, label='Error (%)')\n",
        "\n",
        "# 3.3 Latent Feature Importance\n",
        "ax11 = fig.add_subplot(gs[2, 2])\n",
        "latent_stds = latent_repr.std(axis=0)\n",
        "ax11.bar(range(latent_dim), latent_stds, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "ax11.set_xlabel('Latent Dimension', fontsize=12)\n",
        "ax11.set_ylabel('Std Deviation', fontsize=12)\n",
        "ax11.set_title('Latent Feature Variability', fontsize=12, fontweight='bold')\n",
        "ax11.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3.4 Feature Correlation Heatmap\n",
        "ax12 = fig.add_subplot(gs[2, 3])\n",
        "latent_corr = np.corrcoef(latent_repr.T)\n",
        "im = ax12.imshow(latent_corr, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')\n",
        "ax12.set_xlabel('Latent Dimension', fontsize=12)\n",
        "ax12.set_ylabel('Latent Dimension', fontsize=12)\n",
        "ax12.set_title('Latent Feature Correlations', fontsize=12, fontweight='bold')\n",
        "plt.colorbar(im, ax=ax12, label='Correlation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\u2705 Visualizations complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"AUTOENCODER MODEL - FINAL SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\n\ud83d\udcca ARCHITECTURE:\")\n",
        "print(f\"  Input:      {input_dim} features\")\n",
        "print(f\"  Encoder:    {input_dim} \u2192 {encoder_layers} \u2192 {latent_dim}\")\n",
        "print(f\"  Decoder:    {latent_dim} \u2192 {decoder_layers} \u2192 {input_dim}\")\n",
        "print(f\"  Predictor:  {latent_dim} \u2192 {predictor_layers} \u2192 1\")\n",
        "print(f\"  Total Params: {full_model.count_params():,}\")\n",
        "\n",
        "print(f\"\\n\ud83c\udfaf TRAINING:\")\n",
        "print(f\"  Phase 1 (Autoencoder): {len(history_ae.history['loss'])} epochs\")\n",
        "print(f\"  Phase 2 (Predictor):   {len(history_pred.history['loss'])} epochs\")\n",
        "print(f\"  Training samples:      {len(X_train_scaled):,}\")\n",
        "print(f\"  Validation samples:    {len(X_val_scaled):,}\")\n",
        "print(f\"  Test samples:          {len(X_test_scaled):,}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcc8 PERFORMANCE:\")\n",
        "print(f\"  MAE:           {mae:.0f} W/m\u00b2\")\n",
        "print(f\"  RMSE:          {rmse:.0f} W/m\u00b2\")\n",
        "print(f\"  Within \u00b11%:    {pct_1:.2f}%\")\n",
        "print(f\"  Within \u00b13%:    {pct_3:.2f}%\")\n",
        "print(f\"  Within \u00b15%:    {pct_5:.2f}% {'\u2705' if pct_5 >= 95 else '\u26a0\ufe0f'}  (NASA Target: 95.0%)\")\n",
        "print(f\"  Within \u00b110%:   {pct_10:.2f}%\")\n",
        "print(f\"  Median Error:  {median_err:.2f}%\")\n",
        "print(f\"  95th %ile:     {q95_err:.2f}%\")\n",
        "\n",
        "print(f\"\\n\ud83d\udd2c LATENT SPACE:\")\n",
        "print(f\"  Dimensions:        {latent_dim}\")\n",
        "print(f\"  PCA (2D) Variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
        "print(f\"  Avg Reconstruction Error: {reconstruction_errors.mean():.6f}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udca1 INSIGHTS:\")\n",
        "if pct_5 >= 95:\n",
        "    print(f\"  \u2705 SUCCESS: Autoencoder approach beats NASA baseline!\")\n",
        "    print(f\"  The latent space effectively captures heat flux physics.\")\n",
        "elif pct_5 >= 85:\n",
        "    print(f\"  \ud83d\udcaa STRONG: Close to NASA target ({95-pct_5:.1f}% gap)\")\n",
        "    print(f\"  Consider: Increase latent dim, add more training data, or ensemble models.\")\n",
        "elif pct_5 >= 70:\n",
        "    print(f\"  \ud83d\udcc8 GOOD: Reasonable performance ({95-pct_5:.1f}% gap from target)\")\n",
        "    print(f\"  Consider: Variational autoencoder (VAE), denoising autoencoder, or hybrid approach.\")\n",
        "else:\n",
        "    print(f\"  \ud83d\udd27 NEEDS WORK: {95-pct_5:.1f}% gap from NASA target\")\n",
        "    print(f\"  Consider: Deeper architecture, different latent dim, or combine with physics models.\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcbe MODEL SAVED: best_autoencoder_model.keras\")\n",
        "print(f\"{'='*70}\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}